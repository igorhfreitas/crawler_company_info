{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler Wikipedia\n",
    "\n",
    "Este notebook se refere a um desenvolvimento de um crawler em python para bucar dados da tabela de informações do wikipedia para páginas de empresas.\n",
    "\n",
    "![Infobox da KPMG](img\\Imagem_1.png)\n",
    "\n",
    "### Dados importantes\n",
    "\n",
    "Foi relacionado os seguintes dados como sendo necessários para retorno do Data Frame, excluindo-se os demais campos de informações.\n",
    "\n",
    "- website oficial\n",
    "\n",
    "- sede\n",
    "\n",
    "- pessoas chave \n",
    "\n",
    "- subsidiárias\n",
    "\n",
    "- tipo\n",
    "\n",
    "- proprietario \n",
    "\n",
    "### Busca e retenção da informação\n",
    "\n",
    "O uso da biblioteca pandas é necessário para a criação do Data Frame porém também utilizamos a função <b>read_hmtl()</b>, que consegue ler a página do wikipedia e retornar a tabela requerida já em um Data Frame, ficando muito mais fácil o futuro tratamento dos dados.\n",
    "\n",
    "### Os argumentos do pd.read_html:\n",
    "- header = -1 => necessário pois o infobox de algumas empresas esta formatado de maneira diferente, contendo ou não o logo da empresa, por exemplo https://pt.wikipedia.org/wiki/Alibaba_Group, dessa forma não se corre o risco de deixarmos alguma informação como label das colunas e serem apagadas posteriormente na limpeza dos dados;\n",
    "\n",
    "\n",
    "\n",
    "- attrs = {'class','infobox_v2'} => Segundo a própria wikipedia (ver: https://pt.wikipedia.org/wiki/Ajuda:Infocaixa ) estas tabelas que existem com um resumo de informações sobre o assunto (no caso, empresas) tem por padrão o valor de classe como 'infobox_v2'. desta forma o valor da classe é passado ao pd.read_html para garantir que a tabela que será lida é a requerida.\n",
    "          \n",
    "          \n",
    "![Classe inspecionada da tabela](img\\Imagem_2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A página com a ajuda e padronização da classe inspecionada:\n",
    "\n",
    "![Pagina de ajuda do wikipedia para a classe infobox_v2](img\\Imagem_3.png)\n",
    "\n",
    "          \n",
    "### Problemas\n",
    "\n",
    "- Para páginas em inglês, a classe tem que ser alterada pois a 'infobox_v2' é padronizada para páginas em pt. \n",
    "\n",
    "- Algumas páginas, ex: https://pt.wikipedia.org/wiki/PetroChina_Company, não possuem tabelas informativas (infobox_v2), portanto ao tentar ler a tabela o pandas retorna erro.\n",
    "\n",
    "- Páginas onde a tabela não possui a info \"Razão Social\", o campo do DataFrame é automaticamente preenchido com o nome presente na url da página do wikipedia, ou seja o endereço após o \"wiki/\" (https://pt.wikipedia.org/wiki/<b>Alibaba_Group</b>)\n",
    "\n",
    "### Resultado (Function read_wiki_tables)\n",
    "\n",
    "A função <b>read_wiki_tables</b> irá receber uma lista com todos os endereços das páginas da url do wikipedia, e caso necessário uma lista com as colunas requeridas - as informações principais estão como default, caso seja necessário mais informações será necessário informar a lista como um todo.\n",
    "\n",
    "O output é um DataFrame com as colunas preenchidas e uma lista de erros contendo os links das url que não foi possível obter as informações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A célula abaixo faz a importação dos pacotes/módulos que precisamos, no caso o Pandas para a criação dos DataFrames e leitura da página html e o unidecode para a exclusão de sinais especiais (acentos) dos nomes das colunas.\n",
    "\n",
    "IMPORTANTE:\n",
    "A lista de input required_col deve ser feita sem estes caracteres especiais, ou seja, sem acentos e em caixa baixa. Como o wikipedia é um site colaborativo, a tabela deve seguir um formato padrão para o html (infobox_v2) porém as informações disponíveis nela e suas disposições são flexíveis, sendo que devemos \"fixar\" este parâmetro de comparação com o nome dos tipos de informação requeridos à função em caixa baixa e sem acentuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests\n",
    "import pandas as pd \n",
    "from unidecode import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Empresas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kpmg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ambev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alibaba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Empresas\n",
       "0     kpmg\n",
       "1    ambev\n",
       "2    claro\n",
       "3     vivo\n",
       "4  alibaba"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obj=wikipediaapi.Wikipedia('pt')\n",
    "\n",
    "nome_empresas=pd.read_csv('Base.csv')\n",
    "nome_empresas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos uma lista chamada <b>lista_empresas</b> que conterá a coluna <b>Empresas</b> do Data Frame, esta lista necessita de uma limpeza para a execução da busca no API, pois contém informações muito genéricas que em teste prévio com o API não retornaram resultados satisfatórios, ex:\n",
    "\n",
    "- zf <b>do brasil</b>\n",
    "- bonzano <b>investimentos</b>\n",
    "- bosch brasil <b>fundo de investimento em cotas de fundo de investimento multimercado</b>\n",
    "\n",
    "Estas informações em negrito nos itens acima prejudicam a busca, e muitas vezes retonam resultados nulos. Além disso, haverão muitas informações duplicadas - principalmente após a limpeza das strings - dessa forma é importante excluirmos as strings duplicadas. Estes trechos de string serão excluídos de maneira genérica e manteremos a lista original na list <b>lista_empresas_orig</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_empresas=nome_empresas['Empresas']\n",
    "lista_empresas=list(set(lista_empresas))\n",
    "lista_empresas_orig=lista_empresas\n",
    "lista_empresas_orig.sort()\n",
    "lista_empresas.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_empresas=[x.lower().replace('fundo de investimento em acoes','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('ltda.','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('ltda','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('ltd a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('lt da','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('ltd a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' sa ','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' s a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('fundo de investimento','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('credito privado','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('multimercado','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('s.a.','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('s/ a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('s/a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('s.a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('s.a','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' sa','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' aluminio','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' limitada','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('(brasil)','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace('(brasil )','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().replace(' do brasil','') for x in lista_empresas]\n",
    "lista_empresas=[x.lower().strip(' ') for x in lista_empresas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Function abaixo recebe a lista das empresas, busca a partir da API do wikipedia com a query de \"Prefixsearch\" que busca o melhor match para a palavra buscada. Com a página(s) identificadas, é possível buscar com ajuda do pacote wikipedia-api se a página linkada possui 'empresas' em sua categoria, isso elimina páginas não relacionadas com o objetivo mas que foram resultados de busca (ex: duplicidade) e retorna a url em caso positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_url(lista):\n",
    "    \n",
    "    resultados=pd.DataFrame(columns=['Entity Description','wiki-url','Página não existente'])\n",
    "    url=''\n",
    "    erro_url=False\n",
    "    counter_1=0\n",
    "    S = requests.Session()\n",
    "    \n",
    "    #Pesquisa da página via Wikipedia API\n",
    "    URL = \"https://pt.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    for item in lista:\n",
    "        counter_1+=1\n",
    "        PARAMS = {\n",
    "            'action':\"query\",   \n",
    "            'list':\"prefixsearch\",\n",
    "            'pssearch':item,\n",
    "            'format':\"json\"\n",
    "        }\n",
    "            \n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "            \n",
    "            #Verificação se a página existe em pt\n",
    "        if len(DATA['query']['prefixsearch'])>0:\n",
    "            titulo=DATA['query']['prefixsearch'][0]['title']\n",
    "            if len(titulo)<=0:\n",
    "                erro_url=True\n",
    "                continue\n",
    "            #Verificação se a página tem \"empresas\" nas suas categorias\n",
    "            if len(wiki_obj.page(title=titulo).categories)>=0:\n",
    "                categories_dict=wiki_obj.page(title=titulo).categories\n",
    "                for categ in categories_dict:\n",
    "                    if 'empresas' in categ.lower():\n",
    "                        #Obtem a URL da página\n",
    "                        url=wiki_obj.page(title=titulo).fullurl\n",
    "                        erro_url=False\n",
    "                    else:\n",
    "                        erro_url=True\n",
    "            else:\n",
    "                erro_url=True\n",
    "        else:\n",
    "            erro_url=True\n",
    "        \n",
    "        a={'Entity Description':item,'wiki-url':url,'Página não existente':erro_url}\n",
    "        resultados.loc[len(resultados)+1]=a\n",
    "        url,erro_url='',False\n",
    "        \n",
    "        if counter_1>=10000:\n",
    "            resultados.to_csv('save_temporaro.csv')\n",
    "            print(\"foram buscadas {} páginas\".format(counter_1))\n",
    "            counter_1=0\n",
    "        \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Description</th>\n",
       "      <th>wiki-url</th>\n",
       "      <th>Página não existente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3m</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/3M</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alibaba</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Alibaba_Group</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ambev</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/AMBEV</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bosch brasil  em cotas de</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bozano investimentos</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>claro</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Claro</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>facebook</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Facebook</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kpmg</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/KPMG</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Microsoft</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vivo</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Vivo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zf</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Entity Description                                     wiki-url  \\\n",
       "1                          3m             https://pt.wikipedia.org/wiki/3M   \n",
       "2                     alibaba  https://pt.wikipedia.org/wiki/Alibaba_Group   \n",
       "3                       ambev          https://pt.wikipedia.org/wiki/AMBEV   \n",
       "4   bosch brasil  em cotas de                                                \n",
       "5        bozano investimentos                                                \n",
       "6                       claro          https://pt.wikipedia.org/wiki/Claro   \n",
       "7                    facebook       https://pt.wikipedia.org/wiki/Facebook   \n",
       "8                        kpmg           https://pt.wikipedia.org/wiki/KPMG   \n",
       "9                   microsoft      https://pt.wikipedia.org/wiki/Microsoft   \n",
       "10                       vivo           https://pt.wikipedia.org/wiki/Vivo   \n",
       "11                         zf                                                \n",
       "\n",
       "   Página não existente  \n",
       "1                  True  \n",
       "2                 False  \n",
       "3                  True  \n",
       "4                  True  \n",
       "5                  True  \n",
       "6                  True  \n",
       "7                  True  \n",
       "8                 False  \n",
       "9                  True  \n",
       "10                 True  \n",
       "11                 True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_url=get_wiki_url(lista_empresas)\n",
    "df_wiki_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o resultado para todas as entradas da lista inteira é muito baixo, temos problemas com empresas não existentes no wikipedia, além dos próprios nomes não serem perfeitos para busca. Uma ideia para melhor um pouco o resultado seria buscar no Linkedin. Num pequeno teste, há empresas que não possuem páginas no wiki porém possuem no Linkedin, porém mesmo assim teremos dados faltantes.\n",
    "\n",
    "Agora será possível com este resultado da function get_wiki_url, inputarmos na function abaixo para obter um dataframe com os dados importantes para o enriquecimento da base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wiki_tables(url_list,required_col=['razao social','website oficial','sede','pessoas chave',\n",
    "                                            'subsidiarias','tipo','proprietario'],append_wiki_url=True):\n",
    "    \"\"\"Function to read a list of urls from wikipedia and import to a pandas DataFrame\n",
    "       the infobox of each page.\n",
    "\n",
    "    Args:\n",
    "        url_list (list or str): list of urls\n",
    "        required_col (list): list of data to extract from wikipedia infobox, use lower case and do not use accent \n",
    "                             such as ´,`,^,~\n",
    "        append_wiki_url (Bool): Turn on/off the column wiki_url in DataFrame - used for error analysis\n",
    "\n",
    "    Returns:\n",
    "        df (Pandas DataFrame): A DataFrame contaning all informations from infobox section;\n",
    "        list_of_errors (list): List of wikipedia pages that was impossible to get the infos;\n",
    "    \"\"\"   \n",
    "    \n",
    "    # inicializa o DataFrame e a lista de erro\n",
    "    dfinal=pd.DataFrame()\n",
    "    list_of_errors=[]\n",
    "\n",
    "    if type(url_list)==str: #verifica se foi enviado o input url como lista ou como string\n",
    "        url_list=[url_list]\n",
    "        \n",
    "    for url in url_list:\n",
    "        if url=='':\n",
    "            continue\n",
    "            \n",
    "        #faz a leitura da página html e inputa num DataFrame, se erro a url é enviada para a lista e o loop continua\n",
    "        try:\n",
    "            dfs=pd.read_html(url,header=None,attrs={'class':'infobox_v2','cellpadding':'3'},flavor='bs4')\n",
    "        except ValueError:\n",
    "            list_of_errors.append(url) \n",
    "            continue\n",
    "        \n",
    "        #O read_html retorna uma lista de DataFrames, portanto é necessário pegar a primeira\n",
    "        if len(dfs)>=1:               \n",
    "            df=dfs[0]\n",
    "            try:\n",
    "                df.columns=[0,1]\n",
    "            except ValueError:\n",
    "                list_of_errors.append(url)\n",
    "                continue\n",
    "            \n",
    "            #executa a limpeza dos dados em função das informações inputadas na lista required_col\n",
    "            df.dropna(axis=0,how='all',inplace=True)\n",
    "            df.reset_index(drop=True,inplace=True)           \n",
    "            df[0]=[unidecode(info_type.lower().strip().replace('-',' ')) for info_type in df[0] if type(info_type)==str]\n",
    "            \n",
    "            for idx in range(df.shape[0]):\n",
    "                if df.loc[idx,0] not in required_col:\n",
    "                    df.drop(idx,inplace=True)\n",
    "            \n",
    "            #Transposição dos dados para que a tabela fique em estrutura correta\n",
    "            df=df.T\n",
    "            df.columns=df.loc[0]\n",
    "            df.drop(0,inplace=True)\n",
    "            \n",
    "            #empresas sem o endereço \"razão social\" tem o final da url inputado na variável para não ficar sem valor\n",
    "            if 'razao social' not in df.columns:\n",
    "                df['razao social']=url[30:]\n",
    "            \n",
    "            if append_wiki_url:\n",
    "                df['url']=url\n",
    "\n",
    "            #monta o dataframe final    \n",
    "            dfinal=pd.concat([dfinal,df],sort=False)\n",
    "            \n",
    "        else:\n",
    "            list_of_errors.append(url)\n",
    "\n",
    "    dfinal.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return dfinal,list_of_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para teste da function, foi levantado uma lista de páginas de empresas do wikipedia a partir dos links abaixo:\n",
    "\n",
    "- https://pt.wikipedia.org/wiki/Lista_das_maiores_empresas_do_mundo\n",
    "- https://pt.wikipedia.org/wiki/Lista_dos_maiores_grupos_de_m%C3%ADdia_do_Brasil\n",
    "\n",
    "Com estas tabelas foi montado o arquiov csv \"links_teste_empresas.csv\" que contém todas as url disponíveis nas tabelas destas páginas citadas. Esta lista foi inputada na list <b>links</b> que será usada para rodar a <b>function read_wiki_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Há 8 páginas da wiki para entrada na function.\n"
     ]
    }
   ],
   "source": [
    "links=df_wiki_url[df_wiki_url[\"wiki-url\"]!=\"\"][\"wiki-url\"].to_list()\n",
    "print(\"Há {} páginas da wiki para entrada na function.\".format(len(links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação da função, o tempo para execução da função para 1 url é medido abaixo.\n",
    "\n",
    "A chamada da function é feita informando o dataframe de retorno, a lista de erros para o retorno e com a lista de endereços de url do wikipedia como argumento para a function de crawler, por ex:\n",
    "\n",
    "<b>df,erros=read_wiki_tables(links)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pt.wikipedia.org/wiki/3M',\n",
       " 'https://pt.wikipedia.org/wiki/Alibaba_Group',\n",
       " 'https://pt.wikipedia.org/wiki/AMBEV',\n",
       " 'https://pt.wikipedia.org/wiki/Claro',\n",
       " 'https://pt.wikipedia.org/wiki/Facebook',\n",
       " 'https://pt.wikipedia.org/wiki/KPMG',\n",
       " 'https://pt.wikipedia.org/wiki/Microsoft',\n",
       " 'https://pt.wikipedia.org/wiki/Vivo']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A duração da function é de 1.09 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo</th>\n",
       "      <th>sede</th>\n",
       "      <th>pessoas chave</th>\n",
       "      <th>subsidiarias</th>\n",
       "      <th>website oficial</th>\n",
       "      <th>razao social</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Empresa de capital aberto</td>\n",
       "      <td>Hangzhou, China</td>\n",
       "      <td>Daniel Zhang,(chairman &amp; CEO) Joseph Tsai,(Vic...</td>\n",
       "      <td>Guangzhou Evergrande Football Club, Taobao, Tm...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Alibaba_Group</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Alibaba_Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                       tipo             sede  \\\n",
       "0  Empresa de capital aberto  Hangzhou, China   \n",
       "\n",
       "0                                      pessoas chave  \\\n",
       "0  Daniel Zhang,(chairman & CEO) Joseph Tsai,(Vic...   \n",
       "\n",
       "0                                       subsidiarias website oficial  \\\n",
       "0  Guangzhou Evergrande Football Club, Taobao, Tm...             [1]   \n",
       "\n",
       "0   razao social                                          url  \n",
       "0  Alibaba_Group  https://pt.wikipedia.org/wiki/Alibaba_Group  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "df,erros=read_wiki_tables(links[1])\n",
    "fim = time.time()\n",
    "duracao=fim - inicio\n",
    "\n",
    "print(\"A duração da function é de \" + \"%.2f s.\" % duracao)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a Function funcionando para uma entrada simples, é possível testar com todo o list <b>links</b> de forma a verificar a funcionalidade da Function para páginas diversas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Em caso de incoerência da lista de links com os resutados da function\n",
    "\n",
    "Para garantir que todos os links inputados na Function foram lidos, é possível criar uma lista auxiliar para informar se algum link deixou de ser registrado. Para isso, habilita-se a opção <b>append_wiki_url</b> de forma que seja criada uma coluna com o endereço da página do wikipedia no DataFrame, e atráves de poucas linhas teremos uma saída se todas algum link deixou de ser inputado na lista de erro ou no DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo</th>\n",
       "      <th>sede</th>\n",
       "      <th>website oficial</th>\n",
       "      <th>razao social</th>\n",
       "      <th>url</th>\n",
       "      <th>pessoas chave</th>\n",
       "      <th>subsidiarias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Empresa de capital aberto</td>\n",
       "      <td>Maplewood, Minnesota, Estados Unidos</td>\n",
       "      <td>3m.com</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/3M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Empresa de capital aberto</td>\n",
       "      <td>Hangzhou, China</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Alibaba_Group</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Alibaba_Group</td>\n",
       "      <td>Daniel Zhang,(chairman &amp; CEO) Joseph Tsai,(Vic...</td>\n",
       "      <td>Guangzhou Evergrande Football Club, Taobao, Tm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empresa de capital aberto</td>\n",
       "      <td>São Paulo, SP, Brasil</td>\n",
       "      <td>www.ambev.com.br</td>\n",
       "      <td>Ambev S.A.</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/AMBEV</td>\n",
       "      <td>Bernardo Pinto Paiva, diretor geral[1]</td>\n",
       "      <td>Cervecería y Maltería QuilmesCervecería Nacion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sociedade anônima</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>claro.com.br</td>\n",
       "      <td>Claro S/A</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Claro</td>\n",
       "      <td>Paulo Cesar Teixeira</td>\n",
       "      <td>EmbratelNextel Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Empresa de capital aberto</td>\n",
       "      <td>Menlo Park, Califórnia, Estados Unidos</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Meta, Inc.</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Facebook</td>\n",
       "      <td>Mark Zuckerberg (CEO)Sheryl Sandberg (COO)Davi...</td>\n",
       "      <td>InstagramWhatsAppOculus VR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                       tipo                                    sede  \\\n",
       "0  Empresa de capital aberto    Maplewood, Minnesota, Estados Unidos   \n",
       "1  Empresa de capital aberto                         Hangzhou, China   \n",
       "2  empresa de capital aberto                   São Paulo, SP, Brasil   \n",
       "3          Sociedade anônima                       São Paulo, Brasil   \n",
       "4  Empresa de capital aberto  Menlo Park, Califórnia, Estados Unidos   \n",
       "\n",
       "0   website oficial   razao social  \\\n",
       "0            3m.com             3M   \n",
       "1               [1]  Alibaba_Group   \n",
       "2  www.ambev.com.br     Ambev S.A.   \n",
       "3      claro.com.br      Claro S/A   \n",
       "4      facebook.com     Meta, Inc.   \n",
       "\n",
       "0                                          url  \\\n",
       "0             https://pt.wikipedia.org/wiki/3M   \n",
       "1  https://pt.wikipedia.org/wiki/Alibaba_Group   \n",
       "2          https://pt.wikipedia.org/wiki/AMBEV   \n",
       "3          https://pt.wikipedia.org/wiki/Claro   \n",
       "4       https://pt.wikipedia.org/wiki/Facebook   \n",
       "\n",
       "0                                      pessoas chave  \\\n",
       "0                                                NaN   \n",
       "1  Daniel Zhang,(chairman & CEO) Joseph Tsai,(Vic...   \n",
       "2             Bernardo Pinto Paiva, diretor geral[1]   \n",
       "3                               Paulo Cesar Teixeira   \n",
       "4  Mark Zuckerberg (CEO)Sheryl Sandberg (COO)Davi...   \n",
       "\n",
       "0                                       subsidiarias  \n",
       "0                                                NaN  \n",
       "1  Guangzhou Evergrande Football Club, Taobao, Tm...  \n",
       "2  Cervecería y Maltería QuilmesCervecería Nacion...  \n",
       "3                              EmbratelNextel Brasil  \n",
       "4                         InstagramWhatsAppOculus VR  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste_df,erros_teste=read_wiki_tables(links,append_wiki_url=True)\n",
    "teste_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_df.to_csv('Base_de_dados_merge_wikipedia_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possíveis melhorias\n",
    "\n",
    "A busca das páginas do wikipedia via API por meio do nome das entidades é o principal problema para enriquecimento desta base. Muitas delas não possuem páginas no wikipedia, outras possuem a página porém sem a tabela de informações e estes problemas reduziram muito os resultados.\n",
    "\n",
    "A melhor saída seria fazer a busca das mesmas informações via Google ou LinkedIn.\n",
    "\n",
    "Uma prévia de um possível crawler para o Google Knowledge Panel foi feita, porém sem resultados satisfatórios para uma base grande de dados como a existente (>63k de linhas) pois temos 2 problemas:\n",
    "\n",
    "- O Google KP API não retorna as informações necessárias pois as informações que requeridas que estão presentes no KP não são oficiais, sendo obtidas por meio de crawling ou infos de usuários, postanto o google nao libera via API estas informações.\n",
    "- Utilizando \"Brute-force\" para fazer um crawler para o KP temos problema com o bloqueio de acessos do google e o tempo para executar a função (>10hrs). \n",
    "\n",
    "Para criar um crawler do google KP seria necessário mais horas para desenvolver uma function eficiente. \n",
    "\n",
    "Além da questão supracitada, seria necessário utilizar algumas horas de desenvolvimento para otimizar as functions acima. Como foram feitas de maneira independentes e anexadas a este notebook elas não foram melhoradas, porém é sabido que num suposto ambiente de produção, este crawler não seria muito eficiente.\n",
    "\n",
    "Os pontos listados abaixo são melhorias pequenas e que visam aumentar a efetividade da coleta de informações no wikipedia.\n",
    "\n",
    "É possível analisar páginas em inglês ou outros idiomas para obter um resultado mais rico. Em uma rápida analise de páginas de wikipedia em outros idiomas é possível identificar o mesmo padão de setagens porém com outras classes, como a <b>infobox_vcard</b> para páginas em inglês:\n",
    "\n",
    "![Inspeção do wiki em ingles](img\\Imagem_4.png)\n",
    "\n",
    "\n",
    "Seria necessário um tratamento mais profundo no DataFrame resultante, pois por exemplo a coluna <b>pessoas chave</b> tem valores nulos (NaN na prévia acima) pois algumas páginas não possuem estas informações, porém possuem informações concomitantes, como Proprietário, fundador e diretores.\n",
    "\n",
    "![Wiki Folha da Manhã SP](img\\Imagem_5.png)\n",
    "\n",
    "\n",
    "Seria necessário um tratamento na coluna <b>website oficial</b> para padronização das urls, pois algumas possuem o prefixo \"www\" outras não, assim como um tratamento parecido também seria necessário para coluna <b>subsidiarias</b> pois como é o caso da Folha da Manhã, as suas subsidiárias não estão separadas por nenhum caractere.\n",
    "\n",
    "Por último, a coluna <b>razao social</b> possui casos como o da Alibaba, que por inexistência deste importante campo de identificação na tabela de informação do wikipedia, a function inputa no campo o fim do endereço da URL do wikipedia, de forma que o valor do campo não necessáriamente é a exata razão social da empresa.\n",
    "\n",
    "![Exemplo grupo alibaba.png](img\\Imagem_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
